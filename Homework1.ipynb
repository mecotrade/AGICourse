{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install gensim==4\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from homework1_utils import plot_embeddings, ProgressIndicator, CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version: %s' % sys.version)\n",
    "print('Numpy version: %s' % np.__version__)\n",
    "print('Tensorflow version: %s' % tf.__version__)\n",
    "print('Gensim vesion: %s' % gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Готовые модели языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель этого задания - познакомиться с пакетом `gensim`, предназначенным для работы с векторными моделями.\n",
    "\n",
    "Будем пользоваться моделями на основе Википедии https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.а\n",
    "\n",
    "* Скачать векторное вложение для русского языка по ссылке http://wikipedia2vec.s3.amazonaws.com/models/ru/2018-04-20/ruwiki_20180420_100d.txt.bz2\n",
    "```\n",
    "d=100, window=5, iteration=10, negative=15\n",
    "```\n",
    "* Загрузить модель в память из скачанного файла\n",
    "* В скачанных моделях токены упорядочены по частотности. Однако, некоторые токены не являются словами, а являются сущностиями из википедии. Есть так же и мусор. Необходимо составить список токенов, являющихся словами в том порядке, в котором они возникают в модели\n",
    "* Показать 20 наиболее частых слов и 20 наиболее редких слов\n",
    "* Построить матрицу, состоящую из векторов слов, расположенных в том же порядке, что и слова из предыдущего пункта\n",
    "* Построить гистограмму длин полученных векторов и при необходимости отнормировать их\n",
    "```\n",
    "KeyedVectors.load_word2vec_format\n",
    "KeyedVectors.get_vector\n",
    "KeyedVectors.key_to_index\n",
    "re.compile\n",
    "re.match\n",
    "np.linalg.norm\n",
    "plt.hist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruwiki100 = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_ru = re.compile(r'^[а-яё]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_wiki_ru = # your code\n",
    "embedding_ru = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.б\n",
    "\n",
    "* Скачать векторное вложение для английского языка по ссылке http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.txt.bz2\n",
    "```\n",
    "d=100, window=5, iteration=10, negative=15\n",
    "```\n",
    "* Загрузить модель в память из скачанного файла\n",
    "* В скачанных моделях токены упорядочены по частотности. Однако, некоторые токены не являются словами, а являются сущностиями из википедии. Есть так же и мусор. Необходимо составить список токенов, являющихся словами в том порядке, в котором они возникают в модели\n",
    "* Показать 20 наиболее частых слов и 20 наиболее редких слов\n",
    "* Построить матрицу, состоящую из векторов слов, расположенных в том же порядке, что и слова из предыдущего пункта\n",
    "* Построить гистограмму длин полученных векторов и при необходимости отнормировать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki100 = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_en = re.compile(r'^[a-z]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_wiki_en = # your code\n",
    "embedding_en = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.в\n",
    "* Реализовать метод, решающий задачу аналогий с помощью `gensim`, и выводящий 10 наиболее близких решений\n",
    "* Получить результат для следующих троек слов:\n",
    "  1. король - мужчина + женщиена = королева\n",
    "  1. Москва - Россия + Франция = Париж (все слова приведены к нижнему регистру!)\n",
    "  1. придумать еще несколько аналогий\n",
    "```\n",
    "KeyedVectors.most_similar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, abc_triplets):\n",
    "    \"\"\"\n",
    "    :param model векторная модель слов\n",
    "    :param abc_triplets список триплетов для задачи аналогий, например [('мужчина', 'король', 'женщина'), ('россия', 'москва', 'франция')]\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Матрица перевода, точное решение\n",
    "\n",
    "Цель этого задания - научиться строить точное решения для матрицы перевода при наличии двух наборов нормированных векторов `X` и `Y` для двух языков. Предполагается, что вектора `X[n]` и `Y[n]` для одного и того же `n` соответствуют словам, являющихся взаимными переводами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.a\n",
    "\n",
    "* Загрузить словарь `wiki100_dict` содержащий переводы первых 10000 слов, которые содержатся в модели русского и английского языков\n",
    "* Выделить из него обучающий словарь `train_dict`, включающий первые 3000 слов, и валидационный словарь `val_dict`, включающий следующие 500 слов\n",
    "* Реализовать метод `get_aligned_vectors` использующий произвольный словарь переводов и векторные модели для русского и английского языков, и возвращающий две прямоугольные матрицы `X` и `Y` так, что вектора `X[n]` и `Y[n]` для одного и того же `n` соответствуют словам, являющихся взаимными переводами\n",
    "* Построить матрицы `X` и `Y` для словаря `train_dict`, \n",
    "* Определить функцию `translation_mapping`, которая строит точное решение для ортогональный матрицы переводов используя матрицы `X` и `Y`\n",
    "* Используя метод `plot_embeddings` отобразить проекцию совместного распределения первых 500 слов из `embedding_ru` оставленных без перевода, т.е. \"переведеённых\" с помощью единичной матрицы и `embedding_en`\n",
    "* Используя метод `plot_embeddings` отобразить проекцию совместного распределения первых 500 слов из `embedding_ru` переведеённых с помощью матрицы `W` и `embedding_en` \n",
    "```\n",
    "np.matmul\n",
    "np.linalg.svd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki100_dict = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_size = 3000\n",
    "val_dict_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = # your code\n",
    "val_dict = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aligned_vectors(dictionary):\n",
    "    \"\"\"\n",
    "    :param dictionary словарь слов вида {'король': 'king', 'королева': 'queen', ...}\n",
    "    :return пару матриц оргогональных векторов для русского и английского языков, выровненную согласно словарю\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_aligned_vectors(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_mapping(X, Y):\n",
    "    \"\"\"\n",
    "    :param X матрица оргогональных векторов для русского языка\n",
    "    :param Y матрица оргогональных векторов для английского языка, упорядоченная согласно переводам матрицы `X`\n",
    "    :return точное решение для наилучшего приближения матрицы переводов\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_embeddings(embedding_ru[:500], embedding_en[:500], 'Raw embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_embeddings(np.matmul(embedding_ru[:500], W_train), embedding_en[:500], 'Translated with W_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.б\n",
    "* Реализовать функцию перевода `translate`, которая с помощью точного решения задачи перевода `W`, нормированного вложения `embedding_en` а так же списка слов `words_wiki_en` находит `topn` наиболее близких переводов. Алгоритм работы следующий\n",
    "  1. с помощью модели русского найти вектор, соответствующий слову, и отнормировать его\n",
    "  1. вычислить векторное произведение (равное всилу нормированности векторов косинусному расстоянию) между полученным вектором и всеми векторами вложения `embedding_en`\n",
    "  1. найти индексы для `topn=N` наибольших значений вектороного произведения\n",
    "  1. используя `words_wiki_en` вывести список пар, соответствующий этим индексам вместе со значениями косинусного расстояния\n",
    "```\n",
    "[(слово1, cos1), (слово2, cos2), ..., (словоN, cosN)]\n",
    "```\n",
    "* Реализовать функцию вывода переводов `print_translate`, полученных с помощью функции `translate`, кроме того, она должна показывать, содержится ли слово в обучающем словаре `dictionary`, являющемся одним из параметров. \n",
    "* Привести несколько примеров перевода слов, не содержащихся в обучающем словаре `val_dict` \n",
    "\n",
    "https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "```\n",
    "numpy.argpartition\n",
    "numpy.argsort\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word, W, topn=10):\n",
    "    \"\"\"\n",
    "    :param word слово на русском языке\n",
    "    :param W матрица переводов\n",
    "    :param topn количество вариантов переводов согласно предложенной матрице\n",
    "    :return список пар содержащих перевод слова и косинусное расстояние, например [(перевод1, cos1), (перевод2, cos2)], упорядоченных по убыванию косинусного расстояния\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translate(word, W, topn=10, dictionary=None):\n",
    "    \"\"\"\n",
    "    :param word слово на русском языке\n",
    "    :param W матрица переводов\n",
    "    :param topn количество вариантов переводов согласно предложенной матрице `W`\n",
    "    :param словарь или список слов, которые были использованы при построении матрицы `W`\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.в\n",
    "\n",
    "* Реализовать метод `top_translations`, вычисляющий при помощи валидационного словаря и матрицы переводов `W` словарь переводов, с заданным количеством `topn=N` наиболее близких переводов, использовать метод `translate` полученный в задании **2.б**\n",
    "```\n",
    "{русское слово: [перевод1, перевод2, ..., переводN]}\n",
    "```\n",
    "* Реализовать метод `top_precisions`, вычисляющий словарь переводов с помощью метода  `top_translations`, после чего вычисляющий `Precision@1`, `Precision@2`, ..., `Precision@N` где `Presicion@n` это среднее число попаданий правильного перевода в `topn` из исходного словаря\n",
    "* Построить график зависимости `Presicion@n` от `n` для `train_dict` для `topn=20`\n",
    "\n",
    "```\n",
    "plt.plot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_translations(W, dictionary, topn=20):\n",
    "    \"\"\"\n",
    "    :param W матрица перевода\n",
    "    :param dictionary словарь или список русских слов\n",
    "    :topn количество возможных переводов для каждого русского слова\n",
    "    :return словарь, сопоставляющий каждому русскому слову из `dictionary` список возможных переводов, сделанных согласно матрице `W` и упорядоченный по убыванию косинусного расстояния\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_precisions(W, dictionary, topn=20):\n",
    "    \"\"\"\n",
    "    :param W матрица перевода\n",
    "    :param dictionary словарь или список русских слов\n",
    "    :topn количество возможных переводов для каждого русского слова\n",
    "    :return спискок значений `Precision@n` для `n` от 1 до `topn`\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.г\n",
    "\n",
    "* С помощью валидационного словаря переводов `val_dict` вычислить `Precision@1`, `Precision@2`, ..., `Precision@10` где `Presicion@n` это среднее число попаданий правильного перевода в `topn` для матрицы переводов `W`\n",
    "* Построить график зависимости `Presicion@n` от `n` для обучающего и валидационного словарей `train_dict` и `val_dict` для `topn=20`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Метод последовательных приближений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель этого задания исследовать метод построения матрицы перевода, предложенной в работе [Learning bilingual word embeddings with (almost) no bilingual data, Artetxe et al, 2017](https://www.researchgate.net/publication/318739134_Learning_bilingual_word_embeddings_with_almost_no_bilingual_data). \n",
    "\n",
    "Его особенностью являются очень скромные требования к обучающему словарю, достаточно иметь перевод всего лишь 50 слов! В статье утверждается, что достаточно и 25, но с нашим выборов векторов так не получается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.а\n",
    "\n",
    "* Из первых 1000 слов входящих в словарь `wiki100_dict` выбрать случайным образом подмножество из `seed_dict_size=50` переводов и поместить их в `seed_dict`\n",
    "* Вывести словарь `seed_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.б\n",
    "\n",
    "* На основе словаря `seed_dict` построить пару матриц `X_seed` для русского языка и `Y_seed` для английского языка\n",
    "* Используя полученные матрицы построить матрицу переводов `W_seed` аналогично заданию **2.а**\n",
    "* Построить график `Precision@n` от `n` для матрицы `W_seed` аналогично заданию **2.в**\n",
    "* Используя метод `translate` полученный в задании **2.б** построить 10 наиболее близких переводов матрицей `W_seed` для нескольких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seed, Y_seed = # your code\n",
    "W_seed = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.в"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем создавать синтетический словарь для наиболее частых 10000 русского языка и ограничим количество слов в английском словаре 50000 наиболее употребимыми словами\n",
    "\n",
    "* Реализовать метод `compute_Y`, который получает на вход матрицу переводов, набор векторов для слов русского языка, а так же набор векторов для слов английского языка, и возвращает набор векторов для слов английского языка, являющихся наиболее точными переводами сделанными при помощи матрицы переводов для исходных слов русского языка, переводы вибираем из первых 50000 слов английского языка\n",
    "* Используя метод `compute_Y` вычислить вектора переводов `Y_next` для первых 10000 слов русского языка сделанных матрицей `W_seed`, переводы выбираем из первых 50000 английских слов\n",
    "* Используя вектора первых 10000 слов русского языка и вектора переводов `Y_next` построить новую матрицу переводов `W_next` аналогично заданию `2.а`\n",
    "* Вычислить значения `Precision@n` от `n` для матрицы `W_next` и отобразить их на графике вместе с аналогичными значениями для `W_seed` аналогично заданию **2.в**\n",
    "* Используя метод `translate` полученный в задании **2.б** построить 10 наиболее близких переводов матрицей `W_next` для нескольких слов, **не входящих в `seed_dict`**, сравнить с результататми задания **3.б**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dict_size = 10000\n",
    "target_vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Y(W, ru, en):\n",
    "    \"\"\"\n",
    "    :param W матрица переводов\n",
    "    :param ru матрица векторов русских слов, для которых будет построена матрица векторв соответствующих английских слов\n",
    "    :param en матрица векторов английских слов, из которых будет строиться матрица векторов переводов, т.е. в качестве перевода для русского слов берётся наиболее близкий вектор из этогй матрицы\n",
    "    :return матрица векторов переводов \n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_next = # your code\n",
    "W_next = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.г"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку теперь мы строим систему перевода без учителя, мы не можем пользоваться словарём для валидации. Вместо этого определим расстояние между двумя множествами:\n",
    "1. вектора переведённых русских слов\n",
    "2. вектора слов английского языка\n",
    "\n",
    "Будем ориентироваться на результат задания **2.а**. Расстояние определяем следующим образом:\n",
    "1. для каждой пары векторов, пришедшей из разных множеств, вычисляем косинусное расстояние, получаем матрицу размера `n * m`, где `n` - количество векторов русских слов, `m` - количество векторов английских слов\n",
    "2. для каждой строчки этой матрицы находим максимальное значение, это косинусное расстояние между переведённым русским словом и ближайшим к нему английским словом\n",
    "3. усредняем по всем полученным максимальным значениям\n",
    "\n",
    "* Реализовать метод `similarity` принимающий матрицу переводов, набор векторов слов для русского языка и набор векторов слов для английского языка и вычисляющий степень близости двух наборов согласно описанному алгоритму\n",
    "* Вычислить степень близости для первых 500 слов русского языка и первых 500 слов английского языка для матриц перевода `W_seed`, `W_next` и `W` (полученную в задании **2.а**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(W_ru, en):\n",
    "    \"\"\"\n",
    "    :param W_ru матрица векторов русских слов переведённых с помощью некоторой матрицы переводов\n",
    "    :param en матрица векторов английских слов, не обязательно являющихся переводами русских слов из `W_ru`\n",
    "    :return среднее значения косинусного расстояния до ближайшего английского слова из `en` усреднённое по всем русским словам из `W_ru` \n",
    "    \"\"\"\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.д"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видели в задании **3.в**, следующая итерация матрицы переводов `W_next` обладает лучшими свойствами, чем исходная матрица `W_seed`. Поэтому возникает идея итеративно улучшать качество перевода поочерёдно создавая следующее приближенеи для матрицы векторов слов переводов `Y` и матрицы переводов `W` для фиксированного набора векторов русских слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Реализовать метод итеративного улучшения матрицы переводов начиная с `W_seed` аналогично заданию **3.в**, запустить цикл для 50 итераций. Для каждой итерации мы будем сохранять картинки совместного распределения векторов для переводов русских слов и английских слов. На каждом этапе мы будем вычислять значение близости этих множеств.\n",
    "* Построить значения `Precision@n` от `n` для матрицы перевода последенй итерации и отобразить их на графике вместе с аналогичными значениями для `W_seed`, `W_next` и `W_train` аналогично заданию **2.в**\n",
    "* Используя метод `translate` полученный в задании **2.б** построить 10 наиболее близких переводов матрицей перевода последней итерации для нескольких слов, **не входящих в `seed_dict`**, сравнить с результататми задания **3.б** и **3.в**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = 'iterations_{}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "# figure directory\n",
    "fig_dir = os.path.join('homework1', 'figs', name)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    \n",
    "history = {'iteration': [], 'similarity': [], 'W': []}\n",
    "\n",
    "topn_similarity = 500\n",
    "topn_tnse = 300\n",
    "num_steps = 25  \n",
    "progress = ProgressIndicator(target=num_steps, step_label='Iterations')\n",
    "\n",
    "# first approximation for W\n",
    "W_iter = # your code\n",
    "for iteration in 1 + np.arange(num_steps):\n",
    "    \n",
    "    # build next iteration for Y based on existing W\n",
    "    Y_iter = # your code\n",
    "    # build next iteration for W based on new Y\n",
    "    W_iter = # your code\n",
    "    # compute similarity for new W\n",
    "    similarity_value = # your code\n",
    "    \n",
    "    history['iteration'].append(iteration)\n",
    "    history['W'].append(W_iter)\n",
    "    history['similarity'].append(similarity_value)\n",
    "    clear_output(wait=True)\n",
    "    vectors_tsne_ru, vectors_tsne_en = plot_embeddings(np.matmul(embedding_ru[:topn_tnse], W_iter), embedding_en[:topn_tnse], 'Translated with W at iteration %d' % iteration, os.path.join(fig_dir, '%02d.png' % iteration))\n",
    "    progress.update(iteration, {'Similarity': similarity_value}, {'RU': vectors_tsne_ru, 'EN': vectors_tsne_en})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Состязательное обучение\n",
    "\n",
    "Цель этого задания - построить матрицу переводов приемлимого качества **без использования словаря**. Мы будем действовать в два этапа:\n",
    "1. Сначала построим при помощи состязательного обучения матрицу, которая схватывает некоторые аспекты языка, но все же далека от идеала\n",
    "1. После чего применим метод последовательных приближений для улучшения качества перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.а\n",
    "Используя API пакета `keras` определить \n",
    "* Модель для переводчика, состоящую из \n",
    "  * входящего слоя размерности равной размерности векторов для русского языка, т.е. 100\n",
    "  * выходного слоя размерности равной размерности английского языка, без bias и функции активации\n",
    "  * в качестве инициализации для kernel возьмём единичную матрицу.\n",
    "* Модель для дискриминатора, состоящую из\n",
    "  * входного слоя размерности равной размерности векторов для русского языка, т.е. 100\n",
    "  * скрытого слоя размерности 1024 и активацией `relu`\n",
    "  * выходного размерности 1 с линейной активацией\n",
    "```\n",
    "tf.keras.model.Sequential\n",
    "tf.keras.layers.Input\n",
    "tf.keras.layers.Dense\n",
    "tf.keras.initializers.Constant\n",
    "tf.eye\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во время обучения мы будем использовать следующую схему изменения скорости обучения:\n",
    "1. Во время первых 20000 шагов скорость растёт линейно от 0 до 1e-3\n",
    "1. Во время следующих шагов скорость спадает обратно пропорционально времени обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Custom Learning Rate Schedule', fontsize=24)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(100000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.compile(optimizer=tf.keras.optimizers.Adam(CustomSchedule()))\n",
    "translator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(CustomSchedule()))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.б\n",
    "Используя `embedding_ru` и `embedding_en` построить 2 датасета, выдающего батчи для вложения 5000 наиболее частых русских слов, и для вложений 5000 наиболее частых английских слов. Размер батча 128, и не забыть перемешать слова! \n",
    "```\n",
    "tf.data.Dataset.from_tensor_slices\n",
    "tf.data.Dataset.shuffle\n",
    "tf.data.Dataset.repeat\n",
    "tf.data.Dataset.batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ru = # your code\n",
    "dataset_en = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения мы будем использовать итераторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ru = iter(dataset_ru)\n",
    "iter_en = iter(dataset_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.в\n",
    "Определить метод одного шага совместного обучения переводчика и дискриминатора. \n",
    "* Для построения функции потерь нужно использовать бинарную кросс-энтропию оригинальных английских и переведённых ресских слов, \n",
    "* При обучении переводчика необходимо добавить к кросс-энтропии регуляризацию вида $loss\\_reg = ||1 - W\\cdot W^T||^2$ с коэффициентом `beta/2`, штрафующую матрицу перевода за неортогональность \n",
    "\n",
    "Во время отладки можно закомментировать аннотацию AutoGraph `@tf.function`\n",
    "```\n",
    "tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "tf.function\n",
    "tf.GradientTape\n",
    "tf.GradientTape.gradient\n",
    "tf.keras.optimizers.Optimizer.apply_gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "zeros = tf.zeros(batch_size)\n",
    "ones = tf.ones(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(beta = 1):\n",
    "    # discriminator training\n",
    "    \n",
    "    # predictions_en is discriminator prediction for english word vectors batch\n",
    "    # predictions_ru is discriminator prediction for russian word vectors batch translated with translator\n",
    "    # loss_d is discriminator loss\n",
    "    # your code\n",
    "\n",
    "    # translator training\n",
    "    \n",
    "    # loss_w is translator loss\n",
    "    # loss_reg is regularization loss\n",
    "    # your code\n",
    "    \n",
    "    return prediction_en, prediction_ru, loss_d, loss_w, loss_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.г\n",
    "\n",
    "* Используя состязательное обучение получить историю обучения для матрицы переводов и значения `similarity`, посчитанного для неё. \n",
    "* Найти шаг обучения, при котором занчение `similarity` было максимальным\n",
    "* Вычислить для матрицы переводов полученной на этом шаге, которую мы обозначим `W_adv_best`, значения `Precision@n` от `n` и отобразить их на графике\n",
    "* Используя метод `translate`, полученный в задании **2.б**, для нескольких слов построить 10 наиболее близких переводов матрицей `W_adv_best`\n",
    "\n",
    "**Предупреждение**: На этом шаге состязательное обучение может свалиться в локальной минимум. Признак такой ситуации - значение `similarity` за все время обучения не превышает 0.6. В этом случае нужно вернуться к пункту **4.а** и проделать эксперимент заново"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = 'adversarial_{}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "# figure directory\n",
    "fig_dir = os.path.join('homework1', 'figs', name)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    \n",
    "# model directory\n",
    "model_dir = os.path.join('homework1', 'models', name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "fig_every = 1000\n",
    "save_every = 1000\n",
    "\n",
    "history_adv = {'step': [], 'accuracy_d': [], 'similarity': [], 'loss_reg': [], 'W': []}\n",
    "\n",
    "topn_similarity = 500\n",
    "topn_tnse = 300\n",
    "num_steps = 100000\n",
    "progress = ProgressIndicator(target=num_steps)\n",
    "\n",
    "for step in np.arange(num_steps):\n",
    "    \n",
    "    prediction_en, prediction_ru, loss_d, loss_w, loss_reg = train_step(beta = 10)\n",
    "    \n",
    "    if step % fig_every == 0:\n",
    "        similarity_value = similarity(translator(embedding_ru[:topn_similarity]).numpy(), embedding_en[:topn_similarity])\n",
    "        accuracy_d = (np.mean(prediction_en < 0) + np.mean(prediction_ru > 0)) / 2\n",
    "        history_adv['step'].append(step)\n",
    "        history_adv['accuracy_d'].append(accuracy_d)\n",
    "        history_adv['similarity'].append(similarity_value)\n",
    "        history_adv['loss_reg'].append(loss_reg)\n",
    "        history_adv['W'].append(translator.layers[0].kernel.numpy())\n",
    "        vectors_tsne_ru, vectors_tsne_en = plot_embeddings(translator(embedding_ru[:topn_tnse]).numpy(), embedding_en[:topn_tnse], \n",
    "                                                           'Top %d Translated at Step %d' % (topn_tnse, step), path=os.path.join(fig_dir, '%d.png' % step))\n",
    "        progress.update(step, {'Similarity': similarity_value, 'Discriminator Accuracy': accuracy_d, 'W regularization loss': loss_reg}, {'RU': vectors_tsne_ru, 'EN': vectors_tsne_en})\n",
    "    if step % save_every == 0:\n",
    "        translator.save_weights(os.path.join(model_dir, 'translator-%d.h5' % step))\n",
    "        discriminator.save_weights(os.path.join(model_dir, 'discriminator-%d.h5' % step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_adv_best = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_embeddings(np.matmul(embedding_ru[:500], W_adv_best), embedding_en[:500], 'Translated with W_adv_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4.д\n",
    "\n",
    "* Действуя так же, как и в задании **3.д** улучшить матрицу переводов с помощью метода последовательных приближений, в качестве начального приближения использовать `W_adv_best`\n",
    "* Построить значения `Precision@n` от `n` для матрицы перевода последенй итерации и отобразить их на графике вместе с аналогичными значениями для `W_adv_best` и `W_train`\n",
    "* Используя метод `translate` полученный в задании **2.б**, для нескольких слов построить 10 наиболее близких переводов матрицей перевода последней итерации, сравнить с результатами задания **4.г** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = 'iter_adv{}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "# figure directory\n",
    "fig_dir = os.path.join('homework1', 'figs', name)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    \n",
    "history_iter = {'iteration': [], 'similarity': [], 'W': []}\n",
    "\n",
    "topn_similarity = 500\n",
    "topn_tnse = 300\n",
    "num_steps = 25   \n",
    "progress = ProgressIndicator(target=num_steps, step_label='Iterations')\n",
    "\n",
    "# first approximation for W\n",
    "W_iter = # your code\n",
    "for iteration in 1 + np.arange(num_steps):\n",
    "    \n",
    "    # build next iteration for Y based on existing W\n",
    "    Y_iter = # your code\n",
    "    # build next iteration for W based on new Y\n",
    "    W_iter = # your code\n",
    "    # compute similarity for new W\n",
    "    similarity_value = # your code\n",
    "    \n",
    "    history_iter['iteration'].append(iteration)\n",
    "    history_iter['W'].append(W_iter)\n",
    "    history_iter['similarity'].append(similarity_value)\n",
    "    clear_output(wait=True)\n",
    "    vectors_tsne_ru, vectors_tsne_en = plot_embeddings(np.matmul(embedding_ru[:topn_tnse], W_iter), embedding_en[:topn_tnse], 'Translated with W at iteration %d' % iteration, os.path.join(fig_dir, '%02d.png' % iteration))\n",
    "    progress.update(iteration, {'Similarity': similarity_value}, {'RU': vectors_tsne_ru, 'EN': vectors_tsne_en})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install gensim\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version: %s' % sys.version)\n",
    "print('Numpy version: %s' % np.__version__)\n",
    "print('Tensorflow version: %s' % tf.__version__)\n",
    "print('Googletrans version: %s' % googletrans.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовые модели русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Русский язык лемматизированные слова, d=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачать векторное вложение для русского языка по ссылке https://rusvectores.org/static/models/rusvectores4/unigrams/ruwikiruscorpora-nobigrams_upos_skipgram_300_5_2018.vec.gz\n",
    "```\n",
    "d=300, window=5, skipgram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.a\n",
    "Необходимо реализовать метод подгружающий модель из файла. Файлы в текстовом формате загружаются гораздо дольше, поэтому лучше из сразу переводить в бинарный формат.\n",
    "* если расширение файла `.bin`, т.е. модель в бинарном формате, загрузить и вернуть\n",
    "* если расширение файлп `.vec`, т.е. модель в текстовом формате, то\n",
    "  * проверить, есть ли уже такой же файл с расширением `.bin`, если да, то загрузить и вернуть его\n",
    "  * если нет, то загрузить файл в формате `.vec`\n",
    "  * полученную модель сохранить в бинарном формате, имя файла не меняется, расширение меняетя на `.bin`\n",
    "  * вернуть модель\n",
    "  \n",
    "  \n",
    "```\n",
    "KeyedVectors.load_word2vec_format\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_file):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvectores = get_model(\"\"\"путь к скачанной модели\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.б\n",
    "Построить гистограмму длин векторов для модели и убедиться, что они нормированы\n",
    "```\n",
    "rusvectores.get_vector\n",
    "plt.hist\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.в\n",
    "Вектора в модели упорядочены по частоте. Вывести 20 наиболее частых слов\n",
    "```\n",
    "rusvectores.index2word\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Английский язык, d=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачать вектрное вложение для английского языка по ссылке http://nlp.stanford.edu/data/glove.6B.zip\n",
    "```\n",
    "d=300, GloVe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формат скачанное вложения не совпадает с форматом gensim, необхолдимо преобразовать скачанный файл. Скачанный файл `glove.6B.300d.txt`, преобразованный файл `gensim.glove.6B.300d.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        count += 1\n",
    "gensim_glove_file = open('gensim.glove.6B.300d.txt', 'w', encoding='utf-8')\n",
    "with open('glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    gensim_glove_file.writelines('%d 300\\n' % count)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        gensim_glove_file.writelines(line)\n",
    "gensim_glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove6B = get_model('gensim.glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.г\n",
    "* Убедиться, что вектора в модели GloVe не нормированы построив гистограмму длин векторов\n",
    "* Построить словарь нормированных векторов `{слово: нормированный_вектор}`\n",
    "```\n",
    "np.linalg.norm\n",
    "```\n",
    "* Слова в модели упорядочены по частоте, вывести 20 наиболее частых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove6B_normed = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.д\n",
    "Задача аналогий. Построить решение задачи аналогий с помощью gensim, выводим 10 наиболее близких слов\n",
    "* король - мужчина + женщиена = королева\n",
    "* Москва - Россия + Франция = Париж\n",
    "* придумать еще 10-15 аналогий\n",
    "\n",
    "Описать результат\n",
    "```\n",
    "rusvectores.most_similar\n",
    "glove6B.most_similar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрица перевода, точное решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим словарь с помощью Google Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(filename, source_words, target_words, num_words=500, num_attempts=10, polite_delay=0.25, ban_deplay=10, tagged=True):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        \n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            translations = json.load(file)\n",
    "        offset = len(translations)\n",
    "            \n",
    "    else:\n",
    "        translator = Translator()\n",
    "        pattern_en = re.compile('^[a-z]+$')\n",
    "        pattern_ru = re.compile('^[а-яё]+_.*') if tagged else re.compile('^[а-яё]+')\n",
    "        translations = {}\n",
    "        progbar = tf.keras.utils.Progbar(num_words)\n",
    "        for offset, w in enumerate(source_words):\n",
    "            if pattern_ru.match(w):\n",
    "                # \n",
    "                time.sleep(polite_delay)\n",
    "                success = False\n",
    "                for _ in range(num_attempts):\n",
    "                    try:\n",
    "                        w_en = translator.translate(w.split('_')[0] if tagged else w, src='ru', dest='en').text\n",
    "                        success = True\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(ban_deplay)\n",
    "                assert success, 'After %d attempts translation stil fails' % num_attempts\n",
    "                if pattern_en.match(w_en) and w_en in target_words:\n",
    "                    translations[w] = w_en\n",
    "                    progbar.add(1)\n",
    "            if len(translations) >= num_words:\n",
    "                break\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(translations, file, indent=4)\n",
    "    \n",
    "    return translations, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvectores_dict, rusvectores_offset = create_dict(os.path.join('homework1', 'rusvectores_dict.json'), source_words=rusvectores.index2word, target_words=glove6B.index2words, num_words=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фиксируем значение `rusvectores_offset`, оно нам пригодится!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.a\n",
    "Используя созданный словарь `rusvectores_dict` и векторные модели для русского и английского языков построить точное решение для ортогональный матрицы переводов\n",
    "```\n",
    "np.linalg.svd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.б\n",
    "* Построить функцию перевода, которая с помощью точного решения задачи перевода `W` и модели английского языка находит `topn` наиболее близких переводов. Функция перевода должна показывать, содержится ли слово в тренеровочном словаре `rusvectores_dict`, и корректно обрабатывать случай, когда предложенное слово не содержится в модели `rusvectores`, а значит для него невозможно подобрать векторное представление\n",
    "* Привести несколько примеров перевода слов, не содержащихся в треневорочном словаре `rusvectores_dict`\n",
    "* Построить словарь `translated_dict` {русское слово: [перевод1, перевод2, ..., перевод10]} отображающий каждое русское слово из словаря `rusvectores_dict` в 10 наиболее близких перводов с помощью функции перевода `translate`\n",
    "```\n",
    "rusvectores.get_vector\n",
    "np.dot\n",
    "glove6B.most_similar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word, W, topn=10):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_dict = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.в\n",
    "С помощью словаря переводов, построенного с помощью Google API `rusvectores_dict` вычисляем `Precision@1`, `Precision@2`, ..., `Precision@10`, где `Presicion@n` это среднее число попаданий правильного перевода в top-n из словаря `translated_dict`. Построить график зависимости `Presicion@n` от n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём валидационный словарь, который состоит из слов, не вошедших в словарь, с помощью которого была построена матрица перевода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvectores_val_dict = create_dict(os.path.join('homework1', 'rusvectores_val_dict.json', source_words=rusvectores.index2word[:rusvectores_offset], target_words=glove6B.index2words, num_words=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.г\n",
    "* Для каждого русского слов из словаря `rusvectores_val_dict` с помощью функции `translate` строим словарь `translated_val_dict` с 10 наиболее близкими переводами, аналогично заданию 2.в\n",
    "* С помощью валидационного словаря переводов `rusvectores_val_dict` вычисляем `Precision@1`, `Precision@2`, ..., `Precision@10` \n",
    "* Строим график зависимости `Presicion@n` от n для тренеровочной выборки (значения получены в задании 2.в) и для валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_val_dict = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.д\n",
    "Определить функцию, принимающую 2 набора многомерных векторов одной размерности и отображающее их на двумерной плоскости используя в качестве проекции метод t-SNE. Вектора из различных множеств должны быть окрашены в разные цвета, например, красный и зелёный. Название графика должно задаваться параметром `title`, если задан параметр `path`, график должен сохраняться по заданному пути.\n",
    "```\n",
    "plt.title\n",
    "plt.scatter\n",
    "plt.savefig (не забыть после вызова plt.savefig(path) закрыть картинку вызовом plt.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(vectors_ru, vectors_en, title, path=None):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим на плоскости множество, состоящее из векторов для 500 наиболее частых слов русского языка, и наиболее частых слов английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(embedding_ru[:500], embedding_en[:500], 'Raw embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим на плоскости множество, состоящее из векторов для 500 наиболее частых слов русского языка, **переведённых с помощью матрицы W**, и наиболее частых слов английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(np.dot(embedding_ru[:500], W.T), embedding_en[:500], 'Translated with W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрица перевода, простая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.а\n",
    "Используя keras API создать модель, состоящую из входящего слоя размерности равной размерности векторов для русского языка, и выходного слоя размерности равной размерности английского языка, без bias и функции активации. В качестве инициализации для kernel возьмём единичную матрицу.\n",
    "```\n",
    "tf.keras.model.Sequential\n",
    "tf.keras.layers.Input\n",
    "tf.keras.layers.Dense\n",
    "tf.keras.initializers.Constant\n",
    "tf.eye\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_reg = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_reg.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "translator_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.б\n",
    "Используя словари `rusvectores_dict` и `rusvectores_val_dict` построить тренеровочную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_val = # your code\n",
    "y, y_val = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.в \n",
    "* Обучить модель на построенных выборках, контроллируя среднеквадратичную ошибку на валидационных данных, 10 эпох\n",
    "* Аналогично заданию 2.б определить функцию перевода `translate_reg` использующую модель `translator_reg`\n",
    "* Насколько хорошо работает этот переводчик? Привести примеры удачных и неудачных переводов\n",
    "```\n",
    "tf.keras.model.Model.fit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_reg(word, topn=10):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Состязательное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiki word vectors https://wikipedia2vec.github.io/wikipedia2vec/pretrained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Русский язык, d=100\n",
    "Скачать векторное вложение для русского языка по ссылке http://wikipedia2vec.s3.amazonaws.com/models/ru/2018-04-20/ruwiki_20180420_100d.txt.bz2\n",
    "```\n",
    "d=100, window=5, iteration=10, negative=15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruwiki100 = get_model('ruwiki_20180420_100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Английский язык, d=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачать векторное вложение для английского языка по ссылке http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.txt.bz2\n",
    "```\n",
    "d=100, window=5, iteration=10, negative=15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki100 = get_model('enwiki_20180420_100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.а\n",
    "В скачанных моделях слова упорядочены по частотности. Однако, некоторые слова не являются словами, а являются сущностиями из википедии. Есть так же и мусор. \n",
    "* необходимо для русской и английской википедии составить списки настоящих слов, т.е. для русской википедии слова должны состоять только из кириллических символов, а для английских - только из латиницы. \n",
    "* отдельно составить для каждой из википедий матрицу вложений для отобранных слов\n",
    "* проверить, что вектора нормированы, и, при необходимости, отнормировать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_wiki_ru = # your code\n",
    "words_wiki_en = # your_code\n",
    "embedding_ru = # your_code, embedding_ru.shape = [len(words_wiki_ru), 100]\n",
    "embedding_en = # your_code, embedding_en.shape = [len(words_wiki_en), 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.б\n",
    "Поскольку теперь мы будем строить систему перевода без учителя, мы не можем пользоваться словарём. Вместо этого определим расстояние между двумя множествами:\n",
    "1. вектора переведённых русских слов\n",
    "2. вектора слов английского языка\n",
    "\n",
    "Будем ориентироваться на результат задания 2.д. Расстояние определяем следующим образом:\n",
    "1. для каждой пары векторов, пришедшей из разных множеств, вычисляем косинусное расстояние, получаем матрицу размера n * m, где n - количество векторов русских слов, m - количество векторов английских слов\n",
    "2. для каждой строчки этой матрицы находим максимальное значение, это косинусное расстояние между переведённым русским словом и ближайшим к нему английским словом\n",
    "3. усредняем по всем полученным максимальным значениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(vectors_ru, vectors_en):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.в\n",
    "Используя keras API определить \n",
    "* Модель для дискриминатора, состоящую из\n",
    "  * входного слоя размерности 100\n",
    "  * слоя Dropout с rate=0.1\n",
    "  * скрытого слоя размерности 512 и активацией relu\n",
    "  * выходного размерности 1 с линейной активацией\n",
    "* Модель для переводчика, точно так же, как в Задании 3.а\n",
    "```\n",
    "tf.keras.layers.Dropout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_adv = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_adv.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "translator_adv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.г\n",
    "Используя `embedding_ru` и `embedding_en` полученные в задании 4.а построить 2 датасета, выдающего батчи для вложения 5000 наиболее частых русских слов, и для вложений 5000 наиболее частых английских слов. Размер батча 128, и не забыть перемешать слова! \n",
    "```\n",
    "tf.data.Dataset.from_tensor_slices\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ru = # your code\n",
    "dataset_ru = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения мы будем использовать итераторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ru = iter(dataset_ru)\n",
    "iter_en = iter(dataset_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.д\n",
    "Определить метод одного шага совместного обучения переводчика и дискриминатора. \n",
    "* для построения функции потерь нужно использовать бинарную кросс-энтропии со сглаживанием равным 0.2 для оригинальных английских слов, и бинарную кросс-энтропию без сглаживания для переведённых ресских слов, \n",
    "* при обучении дискриминатора указать для дискриминатора `training=True` чтобы сработала Dropout рагуляризация, При обучении переводчика указываем для дискриминатора `training=False`.\n",
    "* при обучении переводчика необходимо добавить к кросс-энтропии регуляризацию, штрафующую матрицу перевода за неортогональность вида $loss\\_reg = ||1 - W\\cdot W^T||^2$ с коэффициентом `ort_reg`\n",
    "\n",
    "Во время отладки можно закомментировать аннотацию AutoGraph `@tf.function`\n",
    "```\n",
    "tf.keras.losses.BinaryCrossentropy\n",
    "tf.function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "train_step(ort_reg=1):\n",
    "    v_ru = next(iter_ru)\n",
    "    v_en = next(iter_en)\n",
    "    # discriminator training, your code\n",
    "    \n",
    "    vt_ru = next(iter_ru)\n",
    "    # translator training, your code\n",
    "    \n",
    "    # loss_d - discriminator training loss\n",
    "    # loss_t - translator training loss\n",
    "    # loss_reg - non-orthogonality loss, loss_reg = ||1 - W*W.T||^2 (L2 norm)\n",
    "    # prediction_en - discriminator prediction for v_en\n",
    "    # prediction_ru - discriminator prediction for translator_adv(v_ru)\n",
    "    return loss_d, loss_t, loss_reg, prediction_en, prediction_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100000\n",
    "top_val = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.е\n",
    "Обучить модель, из сохранённых весов отобрать значения, при котором `similarity` максимально, и посмотреть несколько вариантов перевода. \n",
    "Может потребоваться несколько запусков, иногда модель сваливается в локальный минимум и дальше не обучается.\n",
    "* С помощью Tensorboard получить графики обучения для `accuracy_d_train` и `similarity`\n",
    "* Аналогично заданию 2.б определить функцию перевода `translate_adv` использующую модель `translator_adv`\n",
    "* Насколько хорошо удалось обучить модель? С какими словами (хотя бы в смысле top 10) она справилась?\n",
    "* Как менялось взаимное расположение множеств векторов английских слов и переведённых русских (использовать сохранённые в процессе тренировки графики)? Какой можно сделать вывод?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = 'adversarial_{}'.format(datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "# write tensorboard readable summary\n",
    "log_dir = 'logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = tf.summary.create_file_writer(os.path.join(log_dir, name))\n",
    "\n",
    "fig_dir = os.path.join('figs', name)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    \n",
    "model_dir = os.path.join('models', name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "log_every = 100\n",
    "fig_every = 1000\n",
    "save_every = 1000\n",
    "\n",
    "history = {'step': [], 'loss_d': [], 'loss_t': [], 'accuracy_d_train': [], 'similarity': [], 'loss_reg': []}\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(n_steps, stateful_metrics=list(history.keys()))\n",
    "step = 0\n",
    "for _ in range(n_steps):\n",
    "    \n",
    "    loss_d, loss_t, loss_reg, prediction_en, prediction_ru = train_step(ort_reg=1)\n",
    "    # log to tensorboard\n",
    "    if step % log_every == 0:\n",
    "        # write tensorboard logs\n",
    "        logs = [\n",
    "            ('loss_d', loss_d.numpy()), \n",
    "            ('loss_t', loss_t.numpy()), \n",
    "            ('accuracy_d_train', (np.mean(prediction_en > 0) + np.mean(prediction_ru < 0)) / 2),\n",
    "            ('similarity', similarity(translator_adv(embedding_ru[:top_val]).numpy(), embedding_en[:top_val])),\n",
    "            ('loss_reg', loss_reg.numpy())\n",
    "        ]\n",
    "        with writer.as_default():\n",
    "            for param, val in logs:\n",
    "                tf.summary.scalar(param, val, step=step)\n",
    "                history[param] += [float(val)]\n",
    "        history['step'] += [step]\n",
    "        progbar.update(step, values=logs)\n",
    "    if step % fig_every == 0:\n",
    "        plot_embeddings(translator_adv(embedding_ru[:top_val]).numpy(), \n",
    "                        embedding_en[:top_val], \n",
    "                        'Top %d Translated at step %d' % (top_val, step), \n",
    "                        path=os.path.join(fig_dir, '%d.png' % step))\n",
    "    if step % save_every == 0:\n",
    "        translator_adv.save_weights(os.path.join(model_dir, 'translator-%d.h5' % step))\n",
    "        discriminator.save_weights(os.path.join(model_dir, 'discriminator-%d.h5' % step))\n",
    "    step += 1\n",
    "progbar.update(step, values=logs)      \n",
    "    \n",
    "# save history as json file\n",
    "with open(os.path.join(log_dir, '%s.json' % name), 'w') as jsonfile:\n",
    "    json.dump({'history': history}, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_adv(word, topn=10):\n",
    "    # your code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.ж\n",
    "* С помощью Google Translate API построить словарь переводов для первых 1000 слов из `words_wiki_ru`\n",
    "* Используя функцию `translate_adv` из задания 4.е построить словарь 20 наиболее близких переводов, сделанных при помощи обученной модели для 1000 первых слов из `words_wiki_ru`\n",
    "* Действуя так же, как в задании 2.в и 2.г построить график `Precision@n` on n для n=1,...,20 и сравнить с графиками, полученными в заданиях 2.в и 2.г"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dict, _ = create_dict(os.path.join('homework1', 'wiki100_dict.json'), source_words=words_wiki_ru, target_words=words_wiki_en, num_words=1000, tagged=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
